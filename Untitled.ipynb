{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "954d291b-9a08-4778-8c1f-0c3612608cb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Plant Disease Detection Starting...\n",
      " Device: CPU\n",
      " PyTorch version: 2.8.0+cpu\n",
      "============================================================\n",
      "ðŸŒ± PLANT DISEASE DETECTION - DEEP LEARNING PIPELINE\n",
      "============================================================\n",
      "\n",
      "ðŸ” Looking for dataset...\n",
      "âœ… Found dataset at: /home/siham/Bureau/xavier/PlantVillage\n",
      "   Classes found: 15\n",
      "   Sample classes: ['Tomato_Leaf_Mold', 'Tomato__Tomato_YellowLeaf__Curl_Virus', 'Tomato_Early_blight', 'Pepper__bell___Bacterial_spot', 'Tomato__Target_Spot']\n",
      "\n",
      " Loading dataset from: /home/siham/Bureau/xavier/PlantVillage\n",
      " Dataset loaded successfully!\n",
      "    Total images: 20638\n",
      "     Number of classes: 15\n",
      "    Classes: ['Pepper__bell___Bacterial_spot', 'Pepper__bell___healthy', 'Potato___Early_blight', 'Potato___Late_blight', 'Potato___healthy', 'Tomato_Bacterial_spot', 'Tomato_Early_blight', 'Tomato_Late_blight', 'Tomato_Leaf_Mold', 'Tomato_Septoria_leaf_spot', 'Tomato_Spider_mites_Two_spotted_spider_mite', 'Tomato__Target_Spot', 'Tomato__Tomato_YellowLeaf__Curl_Virus', 'Tomato__Tomato_mosaic_virus', 'Tomato_healthy']\n",
      "\n",
      "ðŸ“ˆ Class Distribution:\n",
      "   Pepper__bell___Bacterial_spot: 997 images\n",
      "   Pepper__bell___healthy: 1478 images\n",
      "   Potato___Early_blight: 1000 images\n",
      "   Potato___Late_blight: 1000 images\n",
      "   Potato___healthy: 152 images\n",
      "   Tomato_Bacterial_spot: 2127 images\n",
      "   Tomato_Early_blight: 1000 images\n",
      "   Tomato_Late_blight: 1909 images\n",
      "   Tomato_Leaf_Mold: 952 images\n",
      "   Tomato_Septoria_leaf_spot: 1771 images\n",
      "   Tomato_Spider_mites_Two_spotted_spider_mite: 1676 images\n",
      "   Tomato__Target_Spot: 1404 images\n",
      "   Tomato__Tomato_YellowLeaf__Curl_Virus: 3208 images\n",
      "   Tomato__Tomato_mosaic_virus: 373 images\n",
      "   Tomato_healthy: 1591 images\n",
      "\n",
      "âœ‚ï¸  Creating train/validation split (80/20)...\n",
      "    Training images: 16510\n",
      "    Validation images: 4128\n",
      "    Using Albumentations transforms\n",
      "    Batch size: 8\n",
      "    Training batches: 2064\n",
      "   Validation batches: 516\n",
      "ðŸ—ï¸  Creating EfficientNet-B0 model for 15 classes...\n",
      "    Loss function: CrossEntropyLoss\n",
      "    Optimizer: AdamW (lr=0.0001)\n",
      "\n",
      " Starting training for 5 epochs...\n",
      "============================================================\n",
      "\n",
      "ðŸ”„ EPOCH 1/5\n",
      "----------------------------------------\n",
      "\n",
      " Training Epoch 1...\n",
      "  Batch   0/2064 | Loss: 4.5630 | Acc: 0.0%\n",
      "  Batch  10/2064 | Loss: 2.9683 | Acc: 14.8%\n",
      "  Batch  20/2064 | Loss: 1.8327 | Acc: 14.9%\n",
      "  Batch  30/2064 | Loss: 2.1737 | Acc: 18.5%\n",
      "  Batch  40/2064 | Loss: 2.8276 | Acc: 24.1%\n",
      "  Batch  50/2064 | Loss: 1.8719 | Acc: 28.7%\n",
      "  Batch  60/2064 | Loss: 1.9246 | Acc: 30.5%\n",
      "  Batch  70/2064 | Loss: 1.2776 | Acc: 34.0%\n",
      "  Batch  80/2064 | Loss: 1.9621 | Acc: 36.3%\n",
      "  Batch  90/2064 | Loss: 0.8206 | Acc: 38.7%\n",
      "  Batch 100/2064 | Loss: 1.2812 | Acc: 40.6%\n",
      "  Batch 110/2064 | Loss: 1.4762 | Acc: 42.9%\n",
      "  Batch 120/2064 | Loss: 1.0701 | Acc: 45.5%\n",
      "  Batch 130/2064 | Loss: 0.8512 | Acc: 46.9%\n",
      "  Batch 140/2064 | Loss: 1.2065 | Acc: 47.9%\n",
      "  Batch 150/2064 | Loss: 0.7891 | Acc: 49.1%\n",
      "  Batch 160/2064 | Loss: 2.3094 | Acc: 50.3%\n",
      "  Batch 170/2064 | Loss: 1.0416 | Acc: 51.1%\n",
      "  Batch 180/2064 | Loss: 1.0344 | Acc: 51.9%\n",
      "  Batch 190/2064 | Loss: 1.7070 | Acc: 52.7%\n",
      "  Batch 200/2064 | Loss: 1.1272 | Acc: 53.9%\n",
      "  Batch 210/2064 | Loss: 0.8662 | Acc: 54.5%\n",
      "  Batch 220/2064 | Loss: 0.5802 | Acc: 55.2%\n",
      "  Batch 230/2064 | Loss: 1.6274 | Acc: 56.2%\n",
      "  Batch 240/2064 | Loss: 0.9872 | Acc: 57.2%\n",
      "  Batch 250/2064 | Loss: 1.3326 | Acc: 57.7%\n",
      "  Batch 260/2064 | Loss: 0.2054 | Acc: 58.5%\n",
      "  Batch 270/2064 | Loss: 0.3743 | Acc: 58.8%\n",
      "  Batch 280/2064 | Loss: 0.6121 | Acc: 59.4%\n",
      "  Batch 290/2064 | Loss: 0.8278 | Acc: 60.3%\n",
      "  Batch 300/2064 | Loss: 0.4309 | Acc: 60.8%\n",
      "  Batch 310/2064 | Loss: 0.9510 | Acc: 61.2%\n",
      "  Batch 320/2064 | Loss: 0.8258 | Acc: 61.9%\n",
      "  Batch 330/2064 | Loss: 0.3875 | Acc: 62.6%\n",
      "  Batch 340/2064 | Loss: 0.4104 | Acc: 62.9%\n",
      "  Batch 350/2064 | Loss: 1.1018 | Acc: 63.0%\n",
      "  Batch 360/2064 | Loss: 0.6477 | Acc: 63.6%\n",
      "  Batch 370/2064 | Loss: 0.6191 | Acc: 64.1%\n",
      "  Batch 380/2064 | Loss: 1.1961 | Acc: 64.6%\n",
      "  Batch 390/2064 | Loss: 0.2850 | Acc: 64.9%\n",
      "  Batch 400/2064 | Loss: 1.6358 | Acc: 65.3%\n",
      "  Batch 410/2064 | Loss: 0.9179 | Acc: 65.6%\n",
      "  Batch 420/2064 | Loss: 0.6778 | Acc: 65.8%\n",
      "  Batch 430/2064 | Loss: 0.6534 | Acc: 66.1%\n",
      "  Batch 440/2064 | Loss: 0.3607 | Acc: 66.6%\n",
      "  Batch 450/2064 | Loss: 0.1555 | Acc: 67.0%\n",
      "  Batch 460/2064 | Loss: 0.4898 | Acc: 67.4%\n",
      "  Batch 470/2064 | Loss: 0.9043 | Acc: 67.8%\n",
      "  Batch 480/2064 | Loss: 0.6019 | Acc: 68.1%\n",
      "  Batch 490/2064 | Loss: 0.2344 | Acc: 68.5%\n",
      "  Batch 500/2064 | Loss: 0.8555 | Acc: 68.7%\n",
      "  Batch 510/2064 | Loss: 0.1758 | Acc: 69.0%\n",
      "  Batch 520/2064 | Loss: 0.1666 | Acc: 69.2%\n",
      "  Batch 530/2064 | Loss: 0.7919 | Acc: 69.5%\n",
      "  Batch 540/2064 | Loss: 0.2834 | Acc: 69.8%\n",
      "  Batch 550/2064 | Loss: 0.0693 | Acc: 70.2%\n",
      "  Batch 560/2064 | Loss: 0.7696 | Acc: 70.5%\n",
      "  Batch 570/2064 | Loss: 0.5685 | Acc: 70.7%\n",
      "  Batch 580/2064 | Loss: 0.5166 | Acc: 71.1%\n",
      "  Batch 590/2064 | Loss: 0.0423 | Acc: 71.4%\n",
      "  Batch 600/2064 | Loss: 0.2718 | Acc: 71.7%\n",
      "  Batch 610/2064 | Loss: 0.3524 | Acc: 71.8%\n",
      "  Batch 620/2064 | Loss: 0.1660 | Acc: 72.0%\n",
      "  Batch 630/2064 | Loss: 0.0961 | Acc: 72.2%\n",
      "  Batch 640/2064 | Loss: 0.1154 | Acc: 72.3%\n",
      "  Batch 650/2064 | Loss: 0.6791 | Acc: 72.6%\n",
      "  Batch 660/2064 | Loss: 0.7429 | Acc: 72.7%\n",
      "  Batch 670/2064 | Loss: 1.0078 | Acc: 73.0%\n",
      "  Batch 680/2064 | Loss: 0.5376 | Acc: 73.2%\n",
      "  Batch 690/2064 | Loss: 0.1348 | Acc: 73.5%\n",
      "  Batch 700/2064 | Loss: 0.3341 | Acc: 73.5%\n",
      "  Batch 710/2064 | Loss: 0.0291 | Acc: 73.6%\n",
      "  Batch 720/2064 | Loss: 0.3318 | Acc: 73.8%\n",
      "  Batch 730/2064 | Loss: 0.0211 | Acc: 73.9%\n",
      "  Batch 740/2064 | Loss: 0.3721 | Acc: 74.2%\n",
      "  Batch 750/2064 | Loss: 1.1091 | Acc: 74.3%\n",
      "  Batch 760/2064 | Loss: 0.6516 | Acc: 74.5%\n",
      "  Batch 770/2064 | Loss: 0.1154 | Acc: 74.8%\n",
      "  Batch 780/2064 | Loss: 1.4290 | Acc: 74.9%\n",
      "  Batch 790/2064 | Loss: 0.3513 | Acc: 75.1%\n",
      "  Batch 800/2064 | Loss: 0.7972 | Acc: 75.2%\n",
      "  Batch 810/2064 | Loss: 0.2700 | Acc: 75.4%\n",
      "  Batch 820/2064 | Loss: 0.5867 | Acc: 75.6%\n",
      "  Batch 830/2064 | Loss: 0.0875 | Acc: 75.8%\n",
      "  Batch 840/2064 | Loss: 0.5418 | Acc: 75.9%\n",
      "  Batch 850/2064 | Loss: 0.2373 | Acc: 76.0%\n",
      "  Batch 860/2064 | Loss: 0.0140 | Acc: 76.2%\n",
      "  Batch 870/2064 | Loss: 0.4318 | Acc: 76.2%\n",
      "  Batch 880/2064 | Loss: 1.0787 | Acc: 76.3%\n",
      "  Batch 890/2064 | Loss: 0.0679 | Acc: 76.5%\n",
      "  Batch 900/2064 | Loss: 0.1365 | Acc: 76.7%\n",
      "  Batch 910/2064 | Loss: 0.5322 | Acc: 76.9%\n",
      "  Batch 920/2064 | Loss: 0.0066 | Acc: 77.1%\n",
      "  Batch 930/2064 | Loss: 0.8184 | Acc: 77.2%\n",
      "  Batch 940/2064 | Loss: 0.0736 | Acc: 77.4%\n",
      "  Batch 950/2064 | Loss: 0.1730 | Acc: 77.5%\n",
      "  Batch 960/2064 | Loss: 0.1209 | Acc: 77.7%\n",
      "  Batch 970/2064 | Loss: 0.1630 | Acc: 77.8%\n",
      "  Batch 980/2064 | Loss: 0.0614 | Acc: 77.9%\n",
      "  Batch 990/2064 | Loss: 1.0739 | Acc: 78.0%\n",
      "  Batch 1000/2064 | Loss: 0.0595 | Acc: 78.1%\n",
      "  Batch 1010/2064 | Loss: 0.2617 | Acc: 78.2%\n",
      "  Batch 1020/2064 | Loss: 0.1564 | Acc: 78.4%\n",
      "  Batch 1030/2064 | Loss: 0.5624 | Acc: 78.5%\n",
      "  Batch 1040/2064 | Loss: 0.2595 | Acc: 78.6%\n",
      "  Batch 1050/2064 | Loss: 0.0562 | Acc: 78.7%\n",
      "  Batch 1060/2064 | Loss: 0.1211 | Acc: 78.7%\n",
      "  Batch 1070/2064 | Loss: 0.2358 | Acc: 78.8%\n",
      "  Batch 1080/2064 | Loss: 0.2288 | Acc: 78.9%\n",
      "  Batch 1090/2064 | Loss: 0.2772 | Acc: 79.0%\n",
      "  Batch 1100/2064 | Loss: 0.0207 | Acc: 79.1%\n",
      "  Batch 1110/2064 | Loss: 0.0209 | Acc: 79.2%\n",
      "  Batch 1120/2064 | Loss: 0.4182 | Acc: 79.3%\n",
      "  Batch 1130/2064 | Loss: 0.6452 | Acc: 79.4%\n",
      "  Batch 1140/2064 | Loss: 0.0152 | Acc: 79.5%\n",
      "  Batch 1150/2064 | Loss: 0.0245 | Acc: 79.6%\n",
      "  Batch 1160/2064 | Loss: 0.2549 | Acc: 79.7%\n",
      "  Batch 1170/2064 | Loss: 0.2566 | Acc: 79.8%\n",
      "  Batch 1180/2064 | Loss: 0.0610 | Acc: 79.8%\n",
      "  Batch 1190/2064 | Loss: 0.4791 | Acc: 79.9%\n",
      "  Batch 1200/2064 | Loss: 0.2670 | Acc: 80.0%\n",
      "  Batch 1210/2064 | Loss: 0.0501 | Acc: 80.0%\n",
      "  Batch 1220/2064 | Loss: 0.4171 | Acc: 80.1%\n",
      "  Batch 1230/2064 | Loss: 0.2641 | Acc: 80.2%\n",
      "  Batch 1240/2064 | Loss: 0.6027 | Acc: 80.3%\n",
      "  Batch 1250/2064 | Loss: 0.1372 | Acc: 80.4%\n",
      "  Batch 1260/2064 | Loss: 0.2002 | Acc: 80.5%\n",
      "  Batch 1270/2064 | Loss: 0.0601 | Acc: 80.5%\n",
      "  Batch 1280/2064 | Loss: 0.0412 | Acc: 80.6%\n",
      "  Batch 1290/2064 | Loss: 0.0310 | Acc: 80.6%\n",
      "  Batch 1300/2064 | Loss: 0.1813 | Acc: 80.8%\n",
      "  Batch 1310/2064 | Loss: 0.2148 | Acc: 80.9%\n",
      "  Batch 1320/2064 | Loss: 0.8598 | Acc: 81.0%\n",
      "  Batch 1330/2064 | Loss: 0.1108 | Acc: 81.0%\n",
      "  Batch 1340/2064 | Loss: 0.0275 | Acc: 81.1%\n",
      "  Batch 1350/2064 | Loss: 0.2140 | Acc: 81.2%\n",
      "  Batch 1360/2064 | Loss: 0.2580 | Acc: 81.3%\n",
      "  Batch 1370/2064 | Loss: 0.7079 | Acc: 81.3%\n",
      "  Batch 1380/2064 | Loss: 0.2935 | Acc: 81.4%\n",
      "  Batch 1390/2064 | Loss: 0.0276 | Acc: 81.4%\n",
      "  Batch 1400/2064 | Loss: 0.0997 | Acc: 81.5%\n",
      "  Batch 1410/2064 | Loss: 0.4846 | Acc: 81.5%\n",
      "  Batch 1420/2064 | Loss: 0.1165 | Acc: 81.6%\n",
      "  Batch 1430/2064 | Loss: 0.5917 | Acc: 81.7%\n",
      "  Batch 1440/2064 | Loss: 0.0365 | Acc: 81.8%\n",
      "  Batch 1450/2064 | Loss: 0.3405 | Acc: 81.8%\n",
      "  Batch 1460/2064 | Loss: 1.2194 | Acc: 81.9%\n",
      "  Batch 1470/2064 | Loss: 0.2257 | Acc: 82.0%\n",
      "  Batch 1480/2064 | Loss: 0.1912 | Acc: 82.0%\n",
      "  Batch 1490/2064 | Loss: 0.3317 | Acc: 82.0%\n",
      "  Batch 1500/2064 | Loss: 0.0176 | Acc: 82.1%\n",
      "  Batch 1510/2064 | Loss: 0.3772 | Acc: 82.1%\n",
      "  Batch 1520/2064 | Loss: 0.0082 | Acc: 82.2%\n",
      "  Batch 1530/2064 | Loss: 0.1602 | Acc: 82.3%\n",
      "  Batch 1540/2064 | Loss: 0.1374 | Acc: 82.4%\n",
      "  Batch 1550/2064 | Loss: 0.0832 | Acc: 82.4%\n",
      "  Batch 1560/2064 | Loss: 1.1008 | Acc: 82.4%\n",
      "  Batch 1570/2064 | Loss: 0.0547 | Acc: 82.5%\n",
      "  Batch 1580/2064 | Loss: 0.3171 | Acc: 82.6%\n",
      "  Batch 1590/2064 | Loss: 0.0550 | Acc: 82.6%\n",
      "  Batch 1600/2064 | Loss: 0.1498 | Acc: 82.7%\n",
      "  Batch 1610/2064 | Loss: 0.5156 | Acc: 82.7%\n",
      "  Batch 1620/2064 | Loss: 0.0400 | Acc: 82.8%\n",
      "  Batch 1630/2064 | Loss: 0.2086 | Acc: 82.8%\n",
      "  Batch 1640/2064 | Loss: 0.0183 | Acc: 82.9%\n",
      "  Batch 1650/2064 | Loss: 0.1934 | Acc: 83.0%\n",
      "  Batch 1660/2064 | Loss: 0.0487 | Acc: 83.0%\n",
      "  Batch 1670/2064 | Loss: 0.0382 | Acc: 83.1%\n",
      "  Batch 1680/2064 | Loss: 0.1086 | Acc: 83.1%\n",
      "  Batch 1690/2064 | Loss: 0.3856 | Acc: 83.2%\n",
      "  Batch 1700/2064 | Loss: 0.0170 | Acc: 83.2%\n",
      "  Batch 1710/2064 | Loss: 1.4620 | Acc: 83.3%\n",
      "  Batch 1720/2064 | Loss: 0.8482 | Acc: 83.3%\n",
      "  Batch 1730/2064 | Loss: 0.2295 | Acc: 83.4%\n",
      "  Batch 1740/2064 | Loss: 0.4960 | Acc: 83.4%\n",
      "  Batch 1750/2064 | Loss: 0.4004 | Acc: 83.5%\n",
      "  Batch 1760/2064 | Loss: 0.2153 | Acc: 83.5%\n",
      "  Batch 1770/2064 | Loss: 0.0325 | Acc: 83.6%\n",
      "  Batch 1780/2064 | Loss: 0.5536 | Acc: 83.6%\n",
      "  Batch 1790/2064 | Loss: 0.8863 | Acc: 83.7%\n",
      "  Batch 1800/2064 | Loss: 0.6259 | Acc: 83.7%\n",
      "  Batch 1810/2064 | Loss: 0.0676 | Acc: 83.7%\n",
      "  Batch 1820/2064 | Loss: 0.0108 | Acc: 83.8%\n",
      "  Batch 1830/2064 | Loss: 0.0952 | Acc: 83.8%\n",
      "  Batch 1840/2064 | Loss: 0.0628 | Acc: 83.9%\n",
      "  Batch 1850/2064 | Loss: 0.0125 | Acc: 83.9%\n",
      "  Batch 1860/2064 | Loss: 0.0103 | Acc: 84.0%\n",
      "  Batch 1870/2064 | Loss: 0.1148 | Acc: 84.0%\n",
      "  Batch 1880/2064 | Loss: 0.0145 | Acc: 84.1%\n",
      "  Batch 1890/2064 | Loss: 0.2024 | Acc: 84.1%\n",
      "  Batch 1900/2064 | Loss: 0.7345 | Acc: 84.1%\n",
      "  Batch 1910/2064 | Loss: 0.1560 | Acc: 84.2%\n",
      "  Batch 1920/2064 | Loss: 0.1065 | Acc: 84.3%\n",
      "  Batch 1930/2064 | Loss: 0.0190 | Acc: 84.3%\n",
      "  Batch 1940/2064 | Loss: 0.0213 | Acc: 84.4%\n",
      "  Batch 1950/2064 | Loss: 0.0780 | Acc: 84.4%\n",
      "  Batch 1960/2064 | Loss: 1.3912 | Acc: 84.4%\n",
      "  Batch 1970/2064 | Loss: 0.0206 | Acc: 84.5%\n",
      "  Batch 1980/2064 | Loss: 0.0291 | Acc: 84.5%\n",
      "  Batch 1990/2064 | Loss: 0.3761 | Acc: 84.6%\n",
      "  Batch 2000/2064 | Loss: 0.1622 | Acc: 84.6%\n",
      "  Batch 2010/2064 | Loss: 0.2567 | Acc: 84.6%\n",
      "  Batch 2020/2064 | Loss: 0.2711 | Acc: 84.7%\n",
      "  Batch 2030/2064 | Loss: 0.1952 | Acc: 84.7%\n",
      "  Batch 2040/2064 | Loss: 0.0279 | Acc: 84.8%\n",
      "  Batch 2050/2064 | Loss: 0.0176 | Acc: 84.8%\n",
      "  Batch 2060/2064 | Loss: 0.0318 | Acc: 84.9%\n",
      "ðŸ“Š Train Results: Loss=0.4812, Accuracy=0.8486\n",
      " Validating...\n",
      "ðŸ“Š Val Results: Loss=0.0855, Accuracy=0.9743\n",
      " New best model saved! Accuracy: 0.9743\n",
      " Epoch Summary: Train Acc: 0.8486 | Val Acc: 0.9743 | Best: 0.9743\n",
      "\n",
      "ðŸ”„ EPOCH 2/5\n",
      "----------------------------------------\n",
      "\n",
      " Training Epoch 2...\n",
      "  Batch   0/2064 | Loss: 1.1852 | Acc: 87.5%\n",
      "  Batch  10/2064 | Loss: 0.0045 | Acc: 95.5%\n",
      "  Batch  20/2064 | Loss: 0.0083 | Acc: 94.6%\n",
      "  Batch  30/2064 | Loss: 0.0197 | Acc: 95.6%\n",
      "  Batch  40/2064 | Loss: 0.0042 | Acc: 95.1%\n",
      "  Batch  50/2064 | Loss: 0.1567 | Acc: 95.1%\n",
      "  Batch  60/2064 | Loss: 0.0097 | Acc: 95.1%\n",
      "  Batch  70/2064 | Loss: 0.2064 | Acc: 95.1%\n",
      "  Batch  80/2064 | Loss: 0.0228 | Acc: 95.2%\n",
      "  Batch  90/2064 | Loss: 0.3295 | Acc: 95.3%\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "import random\n",
    "import warnings\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "import itertools\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "from torchvision import datasets, transforms\n",
    "import timm\n",
    "\n",
    "try:\n",
    "    import albumentations as A\n",
    "    from albumentations.pytorch import ToTensorV2\n",
    "    import cv2\n",
    "    USE_ALBUMENTATIONS = True\n",
    "except ImportError:\n",
    "    print(\"  Albumentations not found. Using basic transforms.\")\n",
    "    USE_ALBUMENTATIONS = False\n",
    "\n",
    "# Suppress warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "os.environ['NO_ALBUMENTATIONS_UPDATE'] = '1'\n",
    "\n",
    "print(\" Plant Disease Detection Starting...\")\n",
    "print(f\" Device: {'CUDA' if torch.cuda.is_available() else 'CPU'}\")\n",
    "print(f\" PyTorch version: {torch.__version__}\")\n",
    "\n",
    "# ========================== CONFIGURATION ==========================\n",
    "class Config:\n",
    "    # Dataset paths - CHANGE THESE TO MATCH YOUR DATA\n",
    "    DATA_PATHS = [\n",
    "        '/home/siham/Bureau/xavier/PlantVillage',\n",
    "        '/home/siham/Bureau/xavier/PlantVillage/color',\n",
    "        './plantvillage',\n",
    "        './plantvillage/color', \n",
    "        './PlantVillage',\n",
    "        '/content/plantvillage',\n",
    "        '/content/plantvillage/color',\n",
    "        './data',\n",
    "        './dataset'\n",
    "    ]\n",
    "    \n",
    "    # Training settings\n",
    "    BATCH_SIZE = 8  # Small batch for compatibility\n",
    "    IMG_SIZE = 224  # Standard size\n",
    "    NUM_EPOCHS = 5  # Quick demo\n",
    "    BASE_LR = 1e-4\n",
    "    DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    NUM_WORKERS = 2\n",
    "    SEED = 42\n",
    "\n",
    "config = Config()\n",
    "\n",
    "def seed_everything(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "seed_everything(config.SEED)\n",
    "\n",
    "# ========================== FIND DATASET ==========================\n",
    "def find_dataset():\n",
    "    \"\"\"Find the dataset in common locations\"\"\"\n",
    "    print(\"\\nðŸ” Looking for dataset...\")\n",
    "    \n",
    "    for path in config.DATA_PATHS:\n",
    "        if os.path.exists(path):\n",
    "            # Check if it has class subdirectories\n",
    "            try:\n",
    "                subdirs = [d for d in os.listdir(path) \n",
    "                          if os.path.isdir(os.path.join(path, d)) and not d.startswith('.')]\n",
    "                \n",
    "                if len(subdirs) >= 2:  # At least 2 classes\n",
    "                    # Count images in subdirs\n",
    "                    total_images = 0\n",
    "                    for subdir in subdirs[:3]:  # Check first 3 subdirs\n",
    "                        subdir_path = os.path.join(path, subdir)\n",
    "                        images = [f for f in os.listdir(subdir_path) \n",
    "                                if f.lower().endswith(('.jpg', '.jpeg', '.png'))]\n",
    "                        total_images += len(images)\n",
    "                    \n",
    "                    if total_images > 10:  # Found a valid dataset\n",
    "                        print(f\"âœ… Found dataset at: {path}\")\n",
    "                        print(f\"   Classes found: {len(subdirs)}\")\n",
    "                        print(f\"   Sample classes: {subdirs[:5]}\")\n",
    "                        return path\n",
    "            except:\n",
    "                continue\n",
    "    \n",
    "    print(\"âŒ No dataset found! Please:\")\n",
    "    print(\"1. Download PlantVillage dataset\")\n",
    "    print(\"2. Extract it to one of these locations:\")\n",
    "    for path in config.DATA_PATHS:\n",
    "        print(f\"   - {path}\")\n",
    "    print(\"3. Make sure it has subfolders for each disease class\")\n",
    "    return None\n",
    "\n",
    "# ========================== TRANSFORMS ==========================\n",
    "def get_transforms():\n",
    "    \"\"\"Get data transforms - Albumentations if available, else torchvision\"\"\"\n",
    "    if USE_ALBUMENTATIONS:\n",
    "        train_transform = A.Compose([\n",
    "            A.Resize(config.IMG_SIZE, config.IMG_SIZE),\n",
    "            A.HorizontalFlip(p=0.5),\n",
    "            A.VerticalFlip(p=0.3),\n",
    "            A.RandomRotate90(p=0.5),\n",
    "            A.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1, p=0.5),\n",
    "            A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "            ToTensorV2()\n",
    "        ])\n",
    "        \n",
    "        val_transform = A.Compose([\n",
    "            A.Resize(config.IMG_SIZE, config.IMG_SIZE),\n",
    "            A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "            ToTensorV2()\n",
    "        ])\n",
    "        return train_transform, val_transform\n",
    "    else:\n",
    "        train_transform = transforms.Compose([\n",
    "            transforms.Resize((config.IMG_SIZE, config.IMG_SIZE)),\n",
    "            transforms.RandomHorizontalFlip(0.5),\n",
    "            transforms.RandomVerticalFlip(0.3),\n",
    "            transforms.RandomRotation(30),\n",
    "            transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "        ])\n",
    "        \n",
    "        val_transform = transforms.Compose([\n",
    "            transforms.Resize((config.IMG_SIZE, config.IMG_SIZE)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "        ])\n",
    "        return train_transform, val_transform\n",
    "\n",
    "# ========================== CUSTOM DATASET ==========================\n",
    "class PlantDataset(Dataset):\n",
    "    def __init__(self, image_paths, labels, transform=None, use_albumentations=False):\n",
    "        self.image_paths = image_paths\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "        self.use_albumentations = use_albumentations\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        image_path = self.image_paths[idx]\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        if self.use_albumentations and USE_ALBUMENTATIONS:\n",
    "            image = cv2.imread(image_path)\n",
    "            if image is None:\n",
    "                # Fallback - create dummy image\n",
    "                image = np.zeros((224, 224, 3), dtype=np.uint8)\n",
    "            else:\n",
    "                image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "            \n",
    "            if self.transform:\n",
    "                transformed = self.transform(image=image)\n",
    "                image = transformed['image']\n",
    "        else:\n",
    "            from PIL import Image\n",
    "            try:\n",
    "                image = Image.open(image_path).convert('RGB')\n",
    "            except:\n",
    "                image = Image.new('RGB', (224, 224), color='black')\n",
    "            \n",
    "            if self.transform:\n",
    "                image = self.transform(image)\n",
    "        \n",
    "        return image, label\n",
    "\n",
    "# ========================== MODEL ==========================\n",
    "def create_model(num_classes):\n",
    "    \"\"\"Create EfficientNet model\"\"\"\n",
    "    print(f\"ðŸ—ï¸  Creating EfficientNet-B0 model for {num_classes} classes...\")\n",
    "    model = timm.create_model('efficientnet_b0', pretrained=True, num_classes=num_classes)\n",
    "    return model\n",
    "\n",
    "# ========================== TRAINING FUNCTIONS ==========================\n",
    "def train_epoch(model, train_loader, criterion, optimizer, device, epoch):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    print(f\"\\n Training Epoch {epoch}...\")\n",
    "    \n",
    "    for batch_idx, (images, labels) in enumerate(train_loader):\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += labels.size(0)\n",
    "        correct += predicted.eq(labels).sum().item()\n",
    "        \n",
    "        if batch_idx % 10 == 0:\n",
    "            print(f'  Batch {batch_idx:3d}/{len(train_loader)} | '\n",
    "                  f'Loss: {loss.item():.4f} | '\n",
    "                  f'Acc: {100.*correct/total:.1f}%')\n",
    "    \n",
    "    epoch_loss = running_loss / len(train_loader)\n",
    "    epoch_acc = correct / total\n",
    "    print(f\"ðŸ“Š Train Results: Loss={epoch_loss:.4f}, Accuracy={epoch_acc:.4f}\")\n",
    "    return epoch_loss, epoch_acc\n",
    "\n",
    "def validate_epoch(model, val_loader, criterion, device):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    print(\" Validating...\")\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, labels in val_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += labels.size(0)\n",
    "            correct += predicted.eq(labels).sum().item()\n",
    "            \n",
    "            all_preds.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    epoch_loss = running_loss / len(val_loader)\n",
    "    epoch_acc = correct / total\n",
    "    print(f\"ðŸ“Š Val Results: Loss={epoch_loss:.4f}, Accuracy={epoch_acc:.4f}\")\n",
    "    return epoch_loss, epoch_acc, all_preds, all_labels\n",
    "\n",
    "# ========================== PLOTTING FUNCTIONS ==========================\n",
    "def plot_training_curves(history):\n",
    "    \"\"\"Plot training and validation curves\"\"\"\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    \n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(history['train_loss'], label='Train Loss', color='blue')\n",
    "    plt.plot(history['val_loss'], label='Val Loss', color='red')\n",
    "    plt.title('Training & Validation Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(history['train_acc'], label='Train Accuracy', color='blue')\n",
    "    plt.plot(history['val_acc'], label='Val Accuracy', color='red')\n",
    "    plt.title('Training & Validation Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('training_curves.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    print(\"ðŸ“ˆ Training curves saved as 'training_curves.png'\")\n",
    "\n",
    "def plot_confusion_matrix(y_true, y_pred, class_names):\n",
    "    \"\"\"Plot confusion matrix\"\"\"\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    \n",
    "    plt.figure(figsize=(10, 8))\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.colorbar()\n",
    "    \n",
    "    tick_marks = np.arange(len(class_names))\n",
    "    plt.xticks(tick_marks, class_names, rotation=45)\n",
    "    plt.yticks(tick_marks, class_names)\n",
    "    \n",
    "    # Add text annotations\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], 'd'),\n",
    "                horizontalalignment=\"center\",\n",
    "                color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "    \n",
    "    plt.ylabel('True Label')\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('confusion_matrix.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    print(\"ðŸ“Š Confusion matrix saved as 'confusion_matrix.png'\")\n",
    "\n",
    "# ========================== MAIN FUNCTION ==========================\n",
    "def main():\n",
    "    \"\"\"Main training pipeline\"\"\"\n",
    "    print(\"=\" * 60)\n",
    "    print(\"ðŸŒ± PLANT DISEASE DETECTION - DEEP LEARNING PIPELINE\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Find dataset\n",
    "    dataset_path = find_dataset()\n",
    "    if dataset_path is None:\n",
    "        print(\"\\n Cannot proceed without dataset. Please check the instructions above.\")\n",
    "        return\n",
    "    \n",
    "    # Load dataset\n",
    "    print(f\"\\n Loading dataset from: {dataset_path}\")\n",
    "    try:\n",
    "        full_dataset = datasets.ImageFolder(dataset_path)\n",
    "        class_names = full_dataset.classes\n",
    "        num_classes = len(class_names)\n",
    "        \n",
    "        print(f\" Dataset loaded successfully!\")\n",
    "        print(f\"    Total images: {len(full_dataset)}\")\n",
    "        print(f\"     Number of classes: {num_classes}\")\n",
    "        print(f\"    Classes: {class_names}\")\n",
    "        \n",
    "        # Show class distribution\n",
    "        targets = [sample[1] for sample in full_dataset.samples]\n",
    "        class_counts = np.bincount(targets)\n",
    "        print(f\"\\nðŸ“ˆ Class Distribution:\")\n",
    "        for i, (name, count) in enumerate(zip(class_names, class_counts)):\n",
    "            print(f\"   {name}: {count} images\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error loading dataset: {e}\")\n",
    "        return\n",
    "    \n",
    "    # Create train/validation split\n",
    "    print(f\"\\nâœ‚ï¸  Creating train/validation split (80/20)...\")\n",
    "    all_image_paths = [sample[0] for sample in full_dataset.samples]\n",
    "    all_labels = [sample[1] for sample in full_dataset.samples]\n",
    "    \n",
    "    train_paths, val_paths, train_labels, val_labels = train_test_split(\n",
    "        all_image_paths, all_labels, test_size=0.2, \n",
    "        stratify=all_labels, random_state=config.SEED\n",
    "    )\n",
    "    \n",
    "    print(f\"    Training images: {len(train_paths)}\")\n",
    "    print(f\"    Validation images: {len(val_paths)}\")\n",
    "    \n",
    "    # Get transforms\n",
    "    train_transform, val_transform = get_transforms()\n",
    "    print(f\"    Using {'Albumentations' if USE_ALBUMENTATIONS else 'Torchvision'} transforms\")\n",
    "    \n",
    "    # Create datasets\n",
    "    train_dataset = PlantDataset(train_paths, train_labels, train_transform, USE_ALBUMENTATIONS)\n",
    "    val_dataset = PlantDataset(val_paths, val_labels, val_transform, USE_ALBUMENTATIONS)\n",
    "    \n",
    "    # Create data loaders\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset, batch_size=config.BATCH_SIZE, \n",
    "        shuffle=True, num_workers=config.NUM_WORKERS, pin_memory=True\n",
    "    )\n",
    "    val_loader = DataLoader(\n",
    "        val_dataset, batch_size=config.BATCH_SIZE,\n",
    "        shuffle=False, num_workers=config.NUM_WORKERS, pin_memory=True\n",
    "    )\n",
    "    \n",
    "    print(f\"    Batch size: {config.BATCH_SIZE}\")\n",
    "    print(f\"    Training batches: {len(train_loader)}\")\n",
    "    print(f\"   Validation batches: {len(val_loader)}\")\n",
    "    \n",
    "    # Create model\n",
    "    model = create_model(num_classes).to(config.DEVICE)\n",
    "    \n",
    "    # Loss function and optimizer\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=config.BASE_LR, weight_decay=1e-4)\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=config.NUM_EPOCHS)\n",
    "    \n",
    "    print(f\"    Loss function: CrossEntropyLoss\")\n",
    "    print(f\"    Optimizer: AdamW (lr={config.BASE_LR})\")\n",
    "    \n",
    "    # Training loop\n",
    "    print(f\"\\n Starting training for {config.NUM_EPOCHS} epochs...\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    history = defaultdict(list)\n",
    "    best_val_acc = 0.0\n",
    "    \n",
    "    for epoch in range(1, config.NUM_EPOCHS + 1):\n",
    "        print(f\"\\nðŸ”„ EPOCH {epoch}/{config.NUM_EPOCHS}\")\n",
    "        print(\"-\" * 40)\n",
    "        \n",
    "        # Train\n",
    "        train_loss, train_acc = train_epoch(\n",
    "            model, train_loader, criterion, optimizer, config.DEVICE, epoch\n",
    "        )\n",
    "        \n",
    "        # Validate\n",
    "        val_loss, val_acc, val_preds, val_labels = validate_epoch(\n",
    "            model, val_loader, criterion, config.DEVICE\n",
    "        )\n",
    "        \n",
    "        # Update learning rate\n",
    "        scheduler.step()\n",
    "        \n",
    "        # Save metrics\n",
    "        history['train_loss'].append(train_loss)\n",
    "        history['train_acc'].append(train_acc)\n",
    "        history['val_loss'].append(val_loss)\n",
    "        history['val_acc'].append(val_acc)\n",
    "        \n",
    "        # Save best model\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            torch.save({\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'class_names': class_names,\n",
    "                'num_classes': num_classes,\n",
    "                'val_acc': val_acc,\n",
    "                'epoch': epoch\n",
    "            }, 'best_model.pth')\n",
    "            print(f\" New best model saved! Accuracy: {val_acc:.4f}\")\n",
    "        \n",
    "        print(f\" Epoch Summary: Train Acc: {train_acc:.4f} | Val Acc: {val_acc:.4f} | Best: {best_val_acc:.4f}\")\n",
    "    \n",
    "    # Final results\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\" TRAINING COMPLETED!\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\" Best Validation Accuracy: {best_val_acc:.4f} ({best_val_acc*100:.1f}%)\")\n",
    "    print(f\" Best model saved as: best_model.pth\")\n",
    "    \n",
    "    # Generate visualizations\n",
    "    print(f\"\\n Generating visualizations...\")\n",
    "    \n",
    "    # Plot training curves\n",
    "    plot_training_curves(history)\n",
    "    \n",
    "    # Plot confusion matrix\n",
    "    plot_confusion_matrix(val_labels, val_preds, class_names)\n",
    "    \n",
    "    # Classification report\n",
    "    print(f\"\\n Classification Report:\")\n",
    "    print(classification_report(val_labels, val_preds, target_names=class_names))\n",
    "    \n",
    "    print(f\"\\n All done! Check the generated plots and saved model.\")\n",
    "    print(f\" Files generated:\")\n",
    "    print(f\"   - best_model.pth (trained model)\")\n",
    "    print(f\"   - training_curves.png (loss & accuracy plots)\")\n",
    "    print(f\"   - confusion_matrix.png (confusion matrix)\")\n",
    "\n",
    "# ========================== RUN THE PROGRAM ==========================\n",
    "\n",
    "main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (tf-env)",
   "language": "python",
   "name": "tf-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
